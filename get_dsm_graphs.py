#!/usr/bin/env python

import pickle
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
import numpy as np
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
import numpy as np
import scipy.sparse as sp
import networkx as nx
import copy
import argparse
from dgl.nn import SAGEConv
import os
import math
import dgl.function as fn

torch.cuda.set_rng_state(torch.tensor([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 189, 92, 134, 209, 242, 247, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8))

torch.set_rng_state(torch.tensor([164, 101, 208, 133, 211, 83, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 164, 101, 208, 133, 0, 0, 0, 0, 127, 240, 39, 33, 0, 0, 0, 0, 29, 217, 238, 155, 0, 0, 0, 0, 62, 64, 70, 91, 0, 0, 0, 0, 223, 15, 210, 240, 0, 0, 0, 0, 209, 253, 96, 172, 0, 0, 0, 0, 69, 15, 223, 222, 0, 0, 0, 0, 165, 124, 23, 64, 0, 0, 0, 0, 188, 240, 115, 115, 0, 0, 0, 0, 154, 31, 190, 233, 0, 0, 0, 0, 103, 88, 26, 96, 0, 0, 0, 0, 73, 118, 125, 118, 0, 0, 0, 0, 116, 50, 196, 127, 0, 0, 0, 0, 54, 133, 155, 36, 0, 0, 0, 0, 92, 116, 31, 24, 0, 0, 0, 0, 91, 36, 50, 88, 0, 0, 0, 0, 146, 129, 178, 151, 0, 0, 0, 0, 225, 45, 179, 171, 0, 0, 0, 0, 161, 149, 116, 166, 0, 0, 0, 0, 98, 68, 136, 235, 0, 0, 0, 0, 89, 227, 1, 164, 0, 0, 0, 0, 252, 101, 231, 82, 0, 0, 0, 0, 231, 161, 202, 16, 0, 0, 0, 0, 58, 127, 231, 241, 0, 0, 0, 0, 149, 178, 249, 217, 0, 0, 0, 
0, 71, 187, 47, 202, 0, 0, 0, 0, 238, 69, 232, 30, 0, 0, 0, 0, 1, 245, 145, 208, 0, 0, 0, 0, 230, 187, 193, 240, 0, 0, 0, 0, 118, 174, 63, 105, 0, 0, 0, 0, 17, 132, 190, 147, 0, 0, 0, 0, 158, 70, 93, 223, 0, 0, 
0, 0, 17, 225, 225, 52, 0, 0, 0, 0, 214, 228, 6, 18, 0, 0, 0, 0, 144, 206, 8, 87, 0, 0, 0, 0, 88, 24, 252, 222, 0, 0, 0, 0, 11, 79, 251, 240, 0, 0, 0, 0, 77, 118, 169, 25, 0, 0, 0, 0, 135, 225, 69, 136, 0, 0, 0, 0, 160, 38, 229, 111, 0, 0, 0, 0, 173, 102, 123, 193, 0, 0, 0, 0, 207, 160, 100, 150, 0, 0, 0, 0, 11, 38, 92, 17, 0, 0, 0, 0, 130, 229, 3, 216, 0, 0, 0, 0, 17, 149, 226, 1, 0, 0, 0, 0, 226, 232, 161, 63, 0, 0, 
0, 0, 88, 211, 175, 118, 0, 0, 0, 0, 76, 3, 232, 62, 0, 0, 0, 0, 44, 249, 96, 34, 0, 0, 0, 0, 141, 218, 206, 213, 0, 0, 0, 0, 56, 56, 116, 239, 0, 0, 0, 0, 122, 194, 142, 24, 0, 0, 0, 0, 86, 4, 188, 223, 0, 0, 0, 0, 190, 50, 210, 221, 0, 0, 0, 0, 199, 41, 64, 32, 0, 0, 0, 0, 186, 250, 28, 40, 0, 0, 0, 0, 154, 117, 178, 168, 0, 0, 0, 0, 49, 189, 126, 102, 0, 0, 0, 0, 42, 84, 143, 175, 0, 0, 0, 0, 3, 156, 173, 36, 0, 0, 
0, 0, 107, 40, 17, 233, 0, 0, 0, 0, 69, 153, 60, 29, 0, 0, 0, 0, 119, 101, 209, 67, 0, 0, 0, 0, 205, 45, 35, 95, 0, 0, 0, 0, 188, 61, 247, 172, 0, 0, 0, 0, 55, 10, 200, 101, 0, 0, 0, 0, 144, 237, 220, 68, 0, 0, 
0, 0, 120, 83, 67, 17, 0, 0, 0, 0, 156, 38, 131, 191, 0, 0, 0, 0, 155, 202, 186, 116, 0, 0, 0, 0, 8, 89, 84, 134, 0, 0, 0, 0, 57, 123, 51, 202, 0, 0, 0, 0, 42, 168, 215, 41, 0, 0, 0, 0, 219, 210, 57, 60, 0, 0, 0, 0, 177, 99, 164, 220, 0, 0, 0, 0, 133, 151, 19, 216, 0, 0, 0, 0, 42, 126, 123, 104, 0, 0, 0, 0, 68, 202, 106, 230, 0, 0, 0, 0, 81, 205, 82, 138, 0, 0, 0, 0, 14, 109, 209, 131, 0, 0, 0, 0, 12, 114, 79, 32, 0, 0, 0, 0, 13, 107, 180, 113, 0, 0, 0, 0, 14, 168, 203, 102, 0, 0, 0, 0, 62, 85, 178, 119, 0, 0, 0, 0, 47, 89, 179, 145, 0, 0, 0, 0, 22, 68, 182, 226, 0, 0, 0, 0, 159, 25, 235, 180, 0, 0, 0, 0, 72, 32, 193, 32, 0, 
0, 0, 0, 192, 68, 112, 136, 0, 0, 0, 0, 227, 242, 100, 162, 0, 0, 0, 0, 31, 60, 245, 167, 0, 0, 0, 0, 204, 60, 183, 97, 0, 0, 0, 0, 61, 178, 111, 194, 0, 0, 0, 0, 211, 128, 38, 132, 0, 0, 0, 0, 211, 171, 215, 111, 0, 0, 0, 0, 57, 44, 200, 213, 0, 0, 0, 0, 66, 125, 58, 37, 0, 0, 0, 0, 107, 189, 233, 65, 0, 0, 0, 0, 52, 117, 123, 253, 0, 0, 0, 0, 22, 174, 240, 123, 0, 0, 0, 0, 119, 254, 191, 41, 0, 0, 0, 0, 88, 20, 46, 97, 0, 0, 0, 0, 131, 168, 128, 26, 0, 0, 0, 0, 22, 151, 133, 46, 0, 0, 0, 0, 22, 98, 41, 66, 0, 0, 0, 0, 124, 2, 115, 162, 0, 0, 0, 0, 32, 106, 39, 221, 0, 0, 0, 0, 58, 155, 78, 3, 0, 0, 0, 0, 78, 72, 171, 21, 0, 0, 0, 0, 51, 69, 103, 24, 0, 0, 0, 0, 141, 152, 43, 77, 0, 0, 0, 0, 171, 27, 170, 2, 0, 0, 0, 0, 231, 109, 152, 251, 0, 0, 0, 0, 101, 95, 46, 7, 0, 0, 0, 0, 75, 176, 27, 222, 0, 0, 0, 0, 219, 20, 59, 164, 0, 0, 0, 0, 17, 91, 102, 135, 0, 0, 0, 0, 244, 25, 164, 179, 0, 0, 0, 0, 132, 228, 92, 47, 0, 0, 0, 0, 139, 204, 140, 85, 0, 0, 0, 0, 234, 140, 200, 233, 0, 0, 0, 0, 102, 73, 231, 181, 0, 0, 0, 0, 238, 120, 66, 188, 
0, 0, 0, 0, 151, 1, 100, 183, 0, 0, 0, 0, 69, 93, 96, 198, 0, 0, 0, 0, 27, 67, 217, 234, 0, 0, 0, 0, 246, 80, 71, 226, 0, 0, 0, 0, 40, 14, 41, 3, 0, 0, 0, 0, 72, 254, 221, 122, 0, 0, 0, 0, 78, 100, 169, 6, 0, 0, 0, 0, 72, 81, 164, 237, 0, 0, 0, 0, 26, 54, 98, 142, 0, 0, 0, 0, 252, 47, 90, 86, 0, 0, 0, 0, 86, 84, 46, 97, 0, 0, 0, 0, 217, 213, 202, 36, 0, 0, 0, 0, 36, 128, 102, 117, 0, 0, 0, 0, 33, 92, 7, 73, 0, 0, 0, 0, 41, 121, 20, 197, 0, 0, 0, 0, 28, 72, 17, 190, 0, 0, 0, 0, 97, 130, 59, 221, 0, 0, 0, 0, 54, 227, 238, 9, 0, 0, 0, 0, 219, 138, 81, 193, 0, 0, 0, 0, 198, 95, 97, 212, 0, 0, 0, 0, 72, 54, 14, 161, 0, 0, 0, 0, 194, 5, 175, 217, 0, 0, 0, 0, 182, 142, 104, 32, 0, 0, 0, 0, 96, 180, 153, 108, 0, 0, 0, 0, 216, 19, 211, 244, 0, 0, 0, 0, 251, 8, 228, 123, 0, 0, 0, 0, 55, 85, 155, 160, 0, 0, 0, 0, 127, 251, 84, 46, 0, 0, 0, 0, 
178, 48, 151, 54, 0, 0, 0, 0, 210, 120, 147, 223, 0, 0, 0, 0, 14, 132, 141, 154, 0, 0, 0, 0, 86, 133, 211, 97, 0, 0, 0, 0, 238, 42, 48, 36, 0, 0, 0, 0, 130, 78, 124, 162, 0, 0, 0, 0, 29, 121, 141, 198, 0, 0, 0, 
0, 116, 215, 115, 3, 0, 0, 0, 0, 99, 21, 45, 47, 0, 0, 0, 0, 175, 107, 240, 22, 0, 0, 0, 0, 172, 35, 68, 124, 0, 0, 0, 0, 227, 168, 180, 115, 0, 0, 0, 0, 205, 147, 213, 74, 0, 0, 0, 0, 32, 124, 239, 236, 0, 0, 0, 0, 116, 181, 223, 204, 0, 0, 0, 0, 153, 71, 160, 182, 0, 0, 0, 0, 206, 51, 203, 44, 0, 0, 0, 0, 238, 174, 134, 189, 0, 0, 0, 0, 197, 79, 51, 50, 0, 0, 0, 0, 99, 230, 81, 142, 0, 0, 0, 0, 240, 205, 64, 51, 0, 0, 0, 0, 92, 176, 86, 200, 0, 0, 0, 0, 40, 109, 47, 85, 0, 0, 0, 0, 219, 2, 64, 69, 0, 0, 0, 0, 177, 202, 189, 159, 0, 0, 0, 0, 79, 196, 63, 166, 0, 0, 0, 0, 18, 168, 80, 145, 0, 0, 0, 0, 2, 223, 50, 215, 0, 0, 0, 0, 24, 133, 112, 165, 0, 0, 0, 0, 246, 109, 85, 24, 0, 0, 0, 0, 195, 8, 71, 27, 0, 0, 0, 0, 165, 208, 11, 72, 0, 0, 0, 0, 107, 21, 205, 163, 0, 0, 0, 0, 37, 164, 61, 66, 0, 0, 0, 0, 237, 6, 37, 204, 0, 0, 0, 0, 160, 26, 211, 243, 0, 0, 0, 0, 10, 190, 255, 169, 0, 0, 0, 0, 228, 65, 208, 128, 0, 0, 0, 0, 123, 22, 184, 26, 0, 0, 0, 0, 69, 178, 5, 144, 0, 0, 0, 0, 194, 85, 152, 178, 0, 0, 0, 0, 128, 149, 61, 84, 0, 0, 0, 0, 166, 5, 213, 179, 0, 0, 0, 0, 118, 254, 139, 74, 0, 0, 0, 0, 182, 20, 170, 125, 0, 0, 0, 0, 247, 27, 49, 94, 0, 0, 0, 0, 211, 174, 16, 253, 0, 0, 0, 0, 214, 72, 210, 76, 0, 0, 0, 0, 154, 204, 210, 137, 0, 0, 0, 0, 192, 16, 208, 231, 0, 0, 0, 0, 184, 248, 99, 135, 0, 0, 0, 0, 44, 172, 162, 47, 0, 0, 0, 0, 39, 122, 133, 30, 0, 0, 0, 0, 47, 17, 25, 68, 0, 0, 0, 0, 243, 101, 87, 40, 0, 0, 0, 0, 173, 68, 175, 253, 0, 0, 0, 0, 117, 55, 169, 42, 0, 0, 0, 0, 249, 126, 163, 67, 0, 0, 0, 0, 169, 208, 59, 71, 0, 0, 0, 0, 26, 59, 219, 177, 0, 0, 0, 0, 75, 41, 198, 59, 0, 0, 0, 0, 107, 110, 84, 102, 0, 0, 0, 0, 167, 74, 76, 75, 0, 0, 
0, 0, 84, 74, 150, 155, 0, 0, 0, 0, 197, 90, 109, 32, 0, 0, 0, 0, 145, 61, 27, 232, 0, 0, 0, 0, 115, 109, 176, 109, 0, 0, 0, 0, 212, 48, 75, 199, 0, 0, 0, 0, 174, 84, 174, 231, 0, 0, 0, 0, 29, 254, 210, 255, 0, 
0, 0, 0, 179, 80, 14, 115, 0, 0, 0, 0, 24, 25, 179, 90, 0, 0, 0, 0, 188, 72, 198, 220, 0, 0, 0, 0, 59, 235, 97, 200, 0, 0, 0, 0, 249, 197, 10, 132, 0, 0, 0, 0, 233, 111, 16, 42, 0, 0, 0, 0, 208, 216, 190, 191, 0, 0, 0, 0, 190, 237, 17, 78, 0, 0, 0, 0, 64, 4, 135, 121, 0, 0, 0, 0, 139, 119, 82, 189, 0, 0, 0, 0, 244, 122, 64, 228, 0, 0, 0, 0, 91, 179, 255, 31, 0, 0, 0, 0, 208, 118, 90, 194, 0, 0, 0, 0, 41, 205, 12, 91, 0, 0, 0, 0, 179, 89, 239, 65, 0, 0, 0, 0, 38, 166, 76, 166, 0, 0, 0, 0, 33, 209, 34, 92, 0, 0, 0, 0, 142, 162, 134, 51, 0, 0, 0, 0, 245, 32, 254, 186, 0, 0, 0, 0, 99, 49, 168, 224, 0, 0, 0, 0, 209, 219, 103, 126, 0, 0, 0, 0, 2, 10, 76, 54, 0, 0, 0, 0, 189, 5, 105, 59, 0, 0, 0, 0, 133, 105, 172, 136, 0, 0, 0, 0, 56, 226, 47, 7, 0, 0, 0, 0, 14, 57, 124, 69, 0, 0, 0, 0, 226, 138, 244, 199, 0, 0, 0, 0, 189, 52, 244, 121, 0, 0, 0, 0, 37, 107, 181, 142, 0, 0, 0, 0, 93, 38, 252, 197, 0, 0, 0, 0, 17, 114, 149, 193, 0, 0, 0, 0, 22, 164, 127, 16, 0, 0, 0, 0, 171, 131, 197, 149, 0, 0, 0, 0, 171, 99, 1, 176, 0, 0, 0, 0, 172, 195, 128, 52, 0, 0, 0, 0, 220, 63, 56, 137, 0, 0, 0, 0, 151, 1, 113, 164, 0, 0, 0, 0, 203, 93, 129, 65, 0, 0, 0, 0, 181, 27, 193, 219, 0, 0, 0, 0, 210, 85, 254, 148, 0, 0, 0, 0, 21, 44, 244, 252, 0, 0, 0, 0, 180, 43, 135, 244, 0, 0, 0, 0, 58, 47, 186, 53, 0, 0, 0, 0, 234, 172, 80, 152, 0, 0, 0, 0, 145, 96, 180, 222, 0, 0, 0, 0, 164, 60, 214, 160, 0, 0, 0, 0, 137, 196, 132, 203, 0, 0, 0, 0, 126, 101, 85, 240, 0, 0, 0, 0, 94, 240, 107, 7, 0, 0, 0, 0, 36, 36, 202, 235, 0, 0, 0, 0, 114, 35, 42, 165, 0, 0, 0, 0, 64, 236, 166, 238, 0, 0, 0, 0, 128, 18, 32, 54, 0, 0, 0, 0, 146, 205, 13, 6, 0, 0, 0, 0, 173, 61, 115, 253, 0, 0, 0, 0, 186, 116, 59, 194, 0, 0, 0, 0, 18, 15, 251, 175, 0, 0, 0, 0, 102, 130, 140, 242, 0, 0, 0, 0, 240, 127, 250, 15, 0, 0, 0, 0, 200, 234, 219, 27, 0, 0, 0, 0, 1, 170, 224, 121, 0, 0, 0, 0, 26, 19, 157, 246, 0, 0, 0, 0, 248, 234, 223, 108, 0, 0, 0, 0, 89, 246, 229, 65, 0, 0, 0, 0, 213, 73, 247, 236, 0, 0, 0, 0, 140, 168, 237, 32, 0, 0, 0, 0, 91, 108, 202, 216, 0, 0, 0, 0, 216, 215, 63, 242, 0, 0, 0, 0, 136, 93, 177, 8, 0, 0, 0, 0, 202, 
175, 191, 71, 0, 0, 0, 0, 58, 255, 64, 86, 0, 0, 0, 0, 107, 70, 216, 174, 0, 0, 0, 0, 146, 249, 224, 241, 0, 0, 0, 0, 91, 16, 72, 210, 0, 0, 0, 0, 223, 139, 149, 25, 0, 0, 0, 0, 35, 135, 243, 6, 0, 0, 0, 0, 248, 12, 91, 135, 0, 0, 0, 0, 204, 233, 179, 243, 0, 0, 0, 0, 214, 6, 196, 38, 0, 0, 0, 0, 154, 57, 217, 170, 0, 0, 0, 0, 37, 18, 174, 90, 0, 0, 0, 0, 98, 109, 94, 157, 0, 0, 0, 0, 15, 136, 105, 28, 0, 0, 0, 0, 27, 
182, 219, 187, 0, 0, 0, 0, 14, 58, 209, 56, 0, 0, 0, 0, 184, 102, 255, 224, 0, 0, 0, 0, 250, 155, 218, 37, 0, 0, 0, 0, 214, 84, 142, 168, 0, 0, 0, 0, 217, 236, 88, 109, 0, 0, 0, 0, 110, 10, 189, 84, 0, 0, 0, 0, 
2, 134, 51, 182, 0, 0, 0, 0, 56, 223, 9, 31, 0, 0, 0, 0, 81, 10, 226, 65, 0, 0, 0, 0, 202, 226, 226, 253, 0, 0, 0, 0, 136, 11, 96, 149, 0, 0, 0, 0, 174, 104, 215, 221, 0, 0, 0, 0, 126, 226, 187, 165, 0, 0, 0, 0, 42, 184, 184, 112, 0, 0, 0, 0, 54, 173, 156, 128, 0, 0, 0, 0, 196, 42, 237, 66, 0, 0, 0, 0, 250, 77, 216, 154, 0, 0, 0, 0, 26, 124, 216, 154, 0, 0, 0, 0, 187, 206, 121, 118, 0, 0, 0, 0, 166, 26, 198, 12, 0, 0, 
0, 0, 195, 90, 245, 208, 0, 0, 0, 0, 6, 143, 157, 57, 0, 0, 0, 0, 165, 164, 221, 125, 0, 0, 0, 0, 252, 185, 9, 246, 0, 0, 0, 0, 228, 217, 88, 240, 0, 0, 0, 0, 109, 152, 251, 200, 0, 0, 0, 0, 177, 2, 216, 130, 0, 0, 0, 0, 235, 220, 143, 207, 0, 0, 0, 0, 213, 80, 82, 203, 0, 0, 0, 0, 188, 107, 150, 194, 0, 0, 0, 0, 170, 186, 58, 203, 0, 0, 0, 0, 253, 22, 175, 2, 0, 0, 0, 0, 34, 120, 76, 31, 0, 0, 0, 0, 188, 152, 99, 230, 0, 0, 0, 0, 174, 123, 66, 243, 0, 0, 0, 0, 149, 97, 37, 235, 0, 0, 0, 0, 131, 199, 18, 186, 0, 0, 0, 0, 59, 192, 179, 86, 0, 0, 0, 0, 57, 226, 95, 32, 0, 0, 0, 0, 213, 194, 115, 81, 0, 0, 0, 0, 253, 82, 187, 217, 0, 0, 0, 0, 144, 173, 67, 147, 0, 0, 0, 0, 245, 157, 148, 168, 0, 0, 0, 0, 207, 130, 236, 144, 0, 0, 0, 0, 62, 81, 234, 206, 0, 0, 0, 0, 111, 179, 150, 252, 0, 0, 0, 0, 251, 150, 109, 197, 0, 0, 0, 0, 56, 73, 207, 74, 0, 0, 0, 0, 222, 101, 134, 126, 0, 0, 0, 0, 93, 137, 163, 180, 0, 0, 0, 0, 222, 10, 162, 158, 0, 0, 0, 0, 48, 6, 194, 105, 0, 0, 0, 0, 186, 171, 51, 102, 0, 0, 0, 0, 45, 213, 102, 143, 0, 0, 0, 0, 242, 67, 241, 67, 0, 0, 0, 0, 71, 219, 49, 75, 0, 0, 0, 0, 7, 250, 237, 223, 0, 0, 0, 0, 254, 200, 203, 57, 0, 0, 0, 0, 161, 59, 232, 130, 0, 0, 0, 0, 187, 195, 254, 82, 0, 0, 0, 0, 207, 195, 87, 231, 0, 0, 0, 0, 234, 108, 252, 167, 0, 0, 0, 0, 247, 32, 55, 55, 0, 0, 0, 0, 227, 49, 37, 100, 0, 0, 0, 0, 155, 161, 140, 31, 0, 0, 0, 0, 153, 182, 52, 132, 0, 0, 0, 0, 154, 255, 193, 192, 0, 0, 0, 0, 209, 185, 129, 103, 0, 0, 0, 0, 133, 160, 78, 23, 0, 0, 0, 0, 239, 130, 143, 197, 0, 0, 0, 0, 147, 244, 34, 227, 0, 0, 0, 0, 72, 142, 155, 193, 0, 0, 0, 0, 16, 72, 146, 41, 0, 0, 0, 0, 202, 255, 182, 103, 0, 0, 0, 0, 146, 143, 163, 124, 0, 0, 0, 0, 123, 81, 98, 165, 0, 0, 0, 0, 58, 231, 178, 35, 0, 0, 0, 0, 96, 69, 233, 157, 0, 0, 0, 0, 41, 211, 215, 96, 0, 0, 0, 0, 72, 184, 62, 91, 0, 0, 0, 0, 78, 199, 92, 102, 0, 0, 0, 0, 173, 234, 108, 208, 0, 0, 0, 0, 41, 182, 81, 87, 0, 0, 0, 0, 76, 71, 208, 8, 0, 0, 0, 0, 129, 206, 103, 243, 0, 0, 0, 0, 208, 12, 6, 19, 0, 0, 0, 0, 151, 95, 238, 215, 0, 0, 0, 0, 236, 234, 61, 207, 0, 0, 0, 0, 212, 152, 176, 38, 0, 
0, 0, 0, 46, 193, 65, 99, 0, 0, 0, 0, 22, 96, 156, 117, 0, 0, 0, 0, 159, 57, 191, 106, 0, 0, 0, 0, 227, 74, 153, 174, 0, 0, 0, 0, 83, 245, 179, 227, 0, 0, 0, 0, 31, 154, 119, 153, 0, 0, 0, 0, 1, 84, 116, 0, 0, 0, 0, 0, 246, 174, 224, 38, 0, 0, 0, 0, 160, 174, 0, 37, 0, 0, 0, 0, 179, 134, 24, 61, 0, 0, 0, 0, 51, 241, 167, 109, 0, 0, 0, 0, 79, 236, 179, 211, 0, 0, 0, 0, 146, 231, 132, 85, 0, 0, 0, 0, 150, 9, 98, 54, 0, 0, 0, 0, 198, 15, 233, 118, 0, 0, 0, 0, 28, 186, 213, 5, 0, 0, 0, 0, 166, 106, 175, 148, 0, 0, 0, 0, 79, 216, 194, 163, 0, 0, 0, 0, 253, 140, 187, 71, 0, 0, 0, 0, 9, 125, 85, 214, 0, 0, 0, 0, 144, 176, 234, 242, 
0, 0, 0, 0, 158, 86, 27, 71, 0, 0, 0, 0, 91, 69, 125, 33, 0, 0, 0, 0, 136, 17, 7, 138, 0, 0, 0, 0, 20, 199, 242, 239, 0, 0, 0, 0, 182, 220, 244, 188, 0, 0, 0, 0, 168, 104, 163, 143, 0, 0, 0, 0, 183, 70, 33, 179, 0, 0, 0, 0, 15, 196, 234, 197, 0, 0, 0, 0, 99, 198, 221, 36, 0, 0, 0, 0, 183, 65, 95, 107, 0, 0, 0, 0, 119, 84, 185, 234, 0, 0, 0, 0, 110, 103, 124, 8, 0, 0, 0, 0, 17, 174, 112, 40, 0, 0, 0, 0, 97, 199, 18, 48, 0, 0, 0, 0, 242, 147, 194, 99, 0, 0, 0, 0, 141, 107, 150, 12, 0, 0, 0, 0, 80, 229, 193, 227, 0, 0, 0, 0, 111, 230, 125, 232, 0, 0, 0, 0, 77, 182, 239, 246, 0, 0, 0, 0, 120, 172, 68, 153, 0, 0, 0, 0, 213, 87, 187, 103, 0, 0, 0, 0, 88, 28, 182, 5, 0, 0, 0, 0, 109, 72, 108, 156, 0, 0, 0, 0, 129, 252, 132, 120, 0, 0, 0, 0, 55, 32, 24, 159, 0, 0, 0, 0, 161, 19, 52, 235, 0, 0, 0, 0, 163, 114, 123, 139, 0, 0, 0, 0, 63, 100, 
115, 38, 0, 0, 0, 0, 150, 69, 229, 61, 0, 0, 0, 0, 234, 187, 203, 76, 0, 0, 0, 0, 116, 232, 94, 155, 0, 0, 0, 0, 76, 222, 18, 58, 0, 0, 0, 0, 187, 97, 124, 38, 0, 0, 0, 0, 135, 163, 124, 79, 0, 0, 0, 0, 159, 59, 89, 16, 0, 0, 0, 0, 125, 158, 117, 233, 0, 0, 0, 0, 121, 247, 170, 138, 0, 0, 0, 0, 75, 120, 65, 220, 0, 0, 0, 0, 45, 254, 42, 153, 0, 0, 0, 0, 81, 113, 70, 55, 0, 0, 0, 0, 188, 15, 166, 198, 0, 0, 0, 0, 35, 111, 42, 64, 0, 0, 0, 0, 51, 12, 37, 104, 0, 0, 0, 0, 132, 147, 130, 87, 0, 0, 0, 0, 68, 98, 25, 140, 0, 0, 0, 0, 106, 61, 133, 19, 0, 0, 0, 0, 159, 246, 84, 103, 0, 0, 0, 0, 36, 220, 209, 162, 0, 0, 0, 0, 205, 50, 166, 152, 0, 0, 0, 0, 123, 212, 107, 226, 0, 0, 0, 0, 41, 13, 135, 120, 0, 0, 0, 0, 154, 154, 106, 15, 0, 0, 0, 0, 149, 106, 1, 84, 0, 0, 0, 0, 56, 66, 164, 64, 0, 0, 0, 0, 82, 163, 205, 67, 0, 0, 0, 0, 149, 220, 205, 71, 0, 0, 0, 0, 59, 60, 79, 247, 0, 0, 0, 0, 240, 187, 4, 63, 0, 0, 0, 0, 137, 151, 1, 201, 0, 0, 0, 0, 76, 165, 127, 132, 0, 0, 0, 0, 161, 247, 244, 2, 0, 0, 0, 0, 97, 221, 145, 244, 0, 0, 0, 0, 135, 203, 179, 242, 0, 0, 0, 0, 242, 240, 116, 57, 0, 0, 0, 0, 89, 147, 178, 238, 0, 0, 0, 0, 98, 78, 197, 188, 0, 0, 0, 0, 193, 77, 105, 181, 0, 0, 0, 0, 209, 10, 126, 81, 0, 0, 0, 0, 243, 149, 51, 166, 0, 0, 0, 0, 249, 34, 47, 243, 0, 0, 0, 0, 135, 152, 38, 152, 0, 0, 0, 0, 95, 91, 124, 246, 0, 0, 0, 0, 51, 73, 120, 27, 0, 0, 0, 0, 7, 46, 6, 186, 0, 0, 0, 0, 226, 214, 51, 17, 0, 0, 0, 0, 20, 187, 160, 191, 0, 0, 0, 0, 153, 151, 34, 4, 0, 0, 0, 0, 73, 178, 245, 213, 0, 0, 0, 0, 31, 243, 94, 254, 0, 0, 0, 0, 250, 231, 83, 161, 0, 0, 0, 0, 199, 62, 6, 133, 0, 0, 0, 0, 169, 50, 113, 166, 0, 0, 0, 0, 104, 130, 115, 197, 0, 0, 0, 0, 41, 185, 74, 108, 0, 0, 0, 0, 187, 118, 169, 166, 0, 0, 0, 0, 241, 217, 115, 188, 0, 0, 0, 0, 212, 9, 253, 210, 0, 0, 0, 0, 201, 242, 249, 160, 0, 0, 0, 0, 14, 111, 24, 141, 0, 0, 0, 0, 180, 61, 101, 215, 0, 0, 0, 0, 44, 74, 249, 12, 0, 0, 0, 0, 86, 209, 63, 30, 0, 0, 0, 0, 233, 158, 142, 23, 0, 0, 0, 0, 233, 100, 174, 66, 0, 0, 0, 0, 133, 249, 37, 69, 0, 0, 0, 0, 18, 23, 31, 29, 0, 0, 0, 0, 25, 190, 26, 90, 0, 0, 0, 0, 
120, 217, 239, 46, 0, 0, 0, 0, 89, 6, 74, 114, 0, 0, 0, 0, 186, 154, 1, 1, 0, 0, 0, 0, 101, 151, 133, 244, 0, 0, 0, 0, 66, 83, 132, 34, 0, 0, 0, 0, 15, 45, 145, 141, 0, 0, 0, 0, 39, 189, 189, 66, 0, 0, 0, 0, 5, 
248, 30, 19, 0, 0, 0, 0, 1, 137, 21, 2, 0, 0, 0, 0, 110, 152, 215, 131, 0, 0, 0, 0, 166, 240, 148, 31, 0, 0, 0, 0, 137, 201, 21, 199, 0, 0, 0, 0, 126, 95, 57, 239, 0, 0, 0, 0, 94, 147, 39, 110, 0, 0, 0, 0, 137, 
253, 19, 191, 0, 0, 0, 0, 230, 108, 95, 168, 0, 0, 0, 0, 4, 252, 39, 169, 0, 0, 0, 0, 111, 166, 207, 142, 0, 0, 0, 0, 19, 0, 248, 0, 0, 0, 0, 0, 146, 52, 103, 30, 0, 0, 0, 0, 174, 225, 215, 65, 0, 0, 0, 0, 32, 179, 187, 129, 0, 0, 0, 0, 128, 224, 216, 223, 0, 0, 0, 0, 198, 176, 75, 200, 0, 0, 0, 0, 209, 44, 217, 120, 0, 0, 0, 0, 41, 0, 90, 224, 0, 0, 0, 0, 172, 140, 190, 102, 0, 0, 0, 0, 92, 23, 49, 87, 0, 0, 0, 0, 205, 254, 105, 139, 0, 0, 0, 0, 200, 80, 215, 5, 0, 0, 0, 0, 6, 233, 165, 29, 0, 0, 0, 0, 125, 39, 83, 99, 0, 0, 0, 0, 12, 242, 83, 20, 0, 0, 0, 0, 221, 236, 250, 159, 0, 0, 0, 0, 29, 205, 219, 244, 0, 0, 0, 0, 249, 252, 78, 127, 0, 0, 0, 0, 252, 135, 82, 10, 0, 0, 0, 0, 145, 132, 57, 69, 0, 0, 0, 0, 246, 94, 146, 119, 0, 0, 0, 0, 154, 168, 82, 78, 0, 0, 0, 0, 79, 122, 20, 35, 0, 0, 0, 0, 84, 138, 177, 120, 0, 0, 0, 0, 179, 18, 102, 69, 0, 0, 0, 0, 101, 164, 36, 156, 0, 0, 0, 0, 207, 253, 64, 41, 0, 0, 0, 0, 216, 235, 32, 83, 0, 0, 0, 0, 203, 47, 35, 105, 0, 0, 0, 0, 225, 246, 250, 213, 0, 0, 0, 0, 90, 91, 80, 210, 0, 0, 0, 0, 78, 173, 5, 6, 0, 0, 0, 0, 248, 31, 30, 15, 0, 0, 0, 0, 11, 87, 198, 149, 0, 0, 0, 0, 193, 41, 19, 105, 0, 0, 0, 0, 245, 58, 39, 218, 0, 0, 0, 0, 68, 235, 193, 116, 0, 0, 0, 0, 112, 193, 76, 98, 0, 0, 0, 0, 205, 
204, 228, 209, 0, 0, 0, 0, 127, 13, 129, 194, 0, 0, 0, 0, 38, 176, 133, 123, 0, 0, 0, 0, 158, 96, 20, 140, 0, 0, 0, 0, 200, 155, 1, 160, 0, 0, 0, 0, 239, 146, 135, 119, 0, 0, 0, 0, 36, 88, 160, 34, 0, 0, 0, 0, 115, 12, 106, 17, 0, 0, 0, 0, 159, 118, 165, 122, 0, 0, 0, 0, 151, 92, 20, 214, 0, 0, 0, 0, 166, 188, 159, 83, 0, 0, 0, 0, 38, 207, 139, 22, 0, 0, 0, 0, 66, 18, 14, 105, 0, 0, 0, 0, 180, 17, 40, 190, 0, 0, 0, 0, 
20, 101, 67, 187, 0, 0, 0, 0, 245, 169, 73, 254, 0, 0, 0, 0, 86, 182, 190, 44, 0, 0, 0, 0, 55, 248, 43, 251, 0, 0, 0, 0, 206, 194, 152, 89, 0, 0, 0, 0, 246, 164, 46, 193, 0, 0, 0, 0, 245, 51, 97, 12, 0, 0, 0, 0, 246, 158, 218, 174, 0, 0, 0, 0, 146, 76, 253, 67, 0, 0, 0, 0, 78, 227, 238, 124, 0, 0, 0, 0, 123, 247, 13, 173, 0, 0, 0, 0, 14, 102, 65, 144, 0, 0, 0, 0, 14, 177, 189, 195, 0, 0, 0, 0, 116, 209, 241, 16, 0, 0, 
0, 0, 24, 185, 170, 194, 0, 0, 0, 0, 252, 124, 39, 158, 0, 0, 0, 0, 140, 64, 106, 26, 0, 0, 0, 0, 147, 101, 74, 28, 0, 0, 0, 0, 87, 192, 186, 196, 0, 0, 0, 0, 125, 215, 230, 64, 0, 0, 0, 0, 70, 98, 200, 88, 0, 0, 0, 0, 94, 199, 151, 231, 0, 0, 0, 0, 13, 111, 29, 72, 0, 0, 0, 0, 25, 62, 94, 77, 0, 0, 0, 0, 214, 89, 17, 200, 0, 0, 0, 0, 104, 112, 190, 136, 0, 0, 0, 0, 50, 22, 49, 168, 0, 0, 0, 0, 81, 115, 141, 123, 0, 0, 0, 0, 242, 80, 180, 88, 0, 0, 0, 0, 66, 253, 26, 57, 0, 0, 0, 0, 110, 63, 252, 189, 0, 0, 0, 0, 1, 212, 105, 62, 0, 0, 0, 0, 203, 47, 60, 122, 0, 0, 0, 0, 25, 247, 215, 245, 0, 0, 0, 0, 170, 105, 39, 75, 0, 0, 
0, 0, 224, 53, 198, 197, 0, 0, 0, 0, 249, 191, 62, 87, 0, 0, 0, 0, 67, 119, 69, 224, 0, 0, 0, 0, 172, 78, 249, 234, 0, 0, 0, 0, 120, 180, 64, 26, 0, 0, 0, 0, 198, 109, 99, 139, 0, 0, 0, 0, 195, 52, 84, 228, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8))


### below functions and classes are based on https://docs.dgl.ai/tutorials/blitz/4_link_predict.html
def compute_loss(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])
    return F.binary_cross_entropy_with_logits(scores, labels)


def compute_auc(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).cpu().numpy()
    return roc_auc_score(labels, scores)


class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            # Compute a new edge feature named 'score' by a dot-product between the
            # source node feature 'h' and destination node feature 'h'.
            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))
            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.
            return g.edata['score'][:, 0]

class MLPPredictor(nn.Module):
    def __init__(self, h_feats):
        super().__init__()
        self.W1 = nn.Linear(h_feats * 2, h_feats)
        self.W2 = nn.Linear(h_feats, 1)

    def apply_edges(self, edges):
        """
        Computes a scalar score for each edge of the given graph.

        Parameters
        ----------
        edges :
            Has three members ``src``, ``dst`` and ``data``, each of
            which is a dictionary representing the features of the
            source nodes, the destination nodes, and the edges
            themselves.

        Returns
        -------
        dict
            A dictionary of new edge features.
        """
        h = torch.cat([edges.src['h'], edges.dst['h']], 1).cuda()
        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}

    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            g.apply_edges(self.apply_edges)
            return g.edata['score']


class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, 'gcn')
        self.conv2 = SAGEConv(h_feats, h_feats, 'gcn')
        self.conv3 = SAGEConv(h_feats, h_feats, 'gcn')

    def forward(self, g, in_feat):
        #in_feat = in_feat.view(1, -1)
        in_feat = in_feat.cuda()
        h = self.conv1(g.to('cuda:0'), in_feat.cuda())
        h = F.relu(h)
        h = self.conv2(g, h)
        h = F.relu(h)
        h = self.conv3(g, h)
        return h
    
def get_next_subset(full_list, subsize):
    x = [x for x in range(len(full_list))]
    x = np.random.choice(x, subsize)
    
    return [full_list[i] for i in x]

def format_subset(subset, tsize, subsize, full_list):
    dbatch = dgl.batch(subset, ndata=['name-age'])
    dbatch = dbatch.to('cuda:0')
    u, v = dbatch.edges()
    while len(u) != len(v): 
        subset = get_next_subset(full_list, subsize)
        dbatch = dgl.batch(subset, ndata=['name-age'])
        dbatch = dbatch.to('cuda:0')
        u, v = dbatch.edges()
        print(len(u), len(v))
    eids = np.arange(dbatch.number_of_edges())
    eids = np.random.permutation(eids)

    test_size = tsize
    train_size = dbatch.number_of_edges() - test_size
    test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]
    train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]

    adj = sp.coo_matrix((np.ones(len(u)), ((u.cpu()).numpy(), (v.cpu()).numpy())))
    adjtd = adj.todense()
    print(adjtd.shape)
    adj_neg = 1 - adjtd- np.eye(dbatch.number_of_nodes())
    neg_u, neg_v = np.where(adj_neg != 0)
    neg_eids = np.random.choice(len(neg_u), dbatch.number_of_edges() // 2)
    test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]
    train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]
    
    adj = sp.coo_matrix((np.ones(len(u)), ((u.cpu()).numpy(), (v.cpu()).numpy())))
    
    adjtd = adj.todense()
    print(adjtd.shape)
    
    adj_neg = 1 - adjtd - np.eye(dbatch.number_of_nodes())
    neg_u, neg_v = np.where(adj_neg != 0)

    neg_eids = np.random.choice(len(neg_u), dbatch.number_of_edges() // 2)
    test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]
    train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]

    train_g = dgl.remove_edges(dbatch, eids[:test_size])

    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=dbatch.number_of_nodes())
    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=dbatch.number_of_nodes())

    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=dbatch.number_of_nodes())
    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=dbatch.number_of_nodes())
    
    return [train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g]
    
    
    
torch.set_default_tensor_type('torch.cuda.FloatTensor') 
ncsr_age = pd.read_csv('age_subset.csv', index_col=0)
ncsr_vars = pd.read_csv("time_series_vars_ncs2.csv", index_col=0)


parser = argparse.ArgumentParser(description='Run SAGE GNN on DSM Graphs')
parser.add_argument("--dsm-type", type=str, default="GAD",
        help="See /dsm_out")
parser.add_argument("--resample-rate", type=float, default=.3,
        help="% to resample for resample iterations")
parser.add_argument("--iterations", type=int, default=2500,
        help="number of training epochs")
parser.add_argument("--resample-iterations", type=int, default=3,
        help="number of hidden gcn units")
parser.add_argument("--tsize", type=float, default=.1,
        help="number of hidden gcn layers")
parser.add_argument("--comorbid", type=str, default = '',
        help="Weight for L2 loss")
args = parser.parse_args()

dsm_all = []
ncsr_age = pd.read_csv('age_subset.csv', index_col=0)
dsm_type = args.dsm_type
if args.comorbid == '':
    for filename in os.listdir(r'C:\Users\galyn\Documents\GitHub\Seminar\individual_graphs/DSM_' + dsm_type):
        nxg= nx.read_gpickle('individual_graphs/DSM_' + dsm_type + "/"+filename)
        for node in nxg.nodes:
                nxg.nodes[node]['name-age'] = [list(ncsr_age.columns).index(node), nxg.nodes[node]['age']]
        dgl_graph = dgl.from_networkx(nxg, node_attrs = ['name-age'])
        dsm_all.append(dgl_graph)
else: 
    for filename in os.listdir(r'C:\Users\galyn\Documents\GitHub\Seminar\individual_graphs/DSM_' + dsm_type):
        if os.path.exists(r'C:\Users\galyn\Documents\GitHub\Seminar\individual_graphs/DSM_' + args.comorbid + r"\\" + filename):
            nxg= nx.read_gpickle('individual_graphs/DSM_' + dsm_type + "/"+filename)
            for node in nxg.nodes:
                    nxg.nodes[node]['name-age'] = [list(ncsr_age.columns).index(node), nxg.nodes[node]['age']]
            dgl_graph = dgl.from_networkx(nxg, node_attrs = ['name-age'])
            dsm_all.append(dgl_graph)

dsm_type += "_" + args.comorbid
    
static_subsize = int(math.ceil(len(dsm_all)*args.resample_rate))
subset1 =get_next_subset(dsm_all, static_subsize)

dbatch = dgl.batch(subset1, ndata=['name-age'])
dbatch = dbatch.to('cuda:0')

eids = np.arange(dbatch.number_of_edges())
eids = np.random.permutation(eids)
static_tsize = int(math.ceil(len(eids)*args.tsize))

reset_count = 0
reset_model = True
reload_count = 0
while reset_model and reload_count < 5: 
    reset_model = False
    dsm_subset = get_next_subset(dsm_all, static_subsize)


    train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = format_subset(dsm_subset, static_tsize, static_subsize, dsm_all)


    model = GraphSAGE(train_g.ndata['name-age'].shape[1], 1)
    # You can replace DotPredictor with MLPPredictor.
    #pred = MLPPredictor(1)
    pred = DotPredictor()


    auc = [] 
    loss_arr = []
    sample_iter = 0
    optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)
    x =0
    while x < args.resample_iterations:
        if len(auc) == 0: 
            auc.append([])
            loss_arr.append([])
        else: 
            dsm_subset = get_next_subset(dsm_all, static_subsize)
            train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = format_subset(dsm_subset, static_tsize, static_subsize, dsm_all)
            sample_iter +=1
            auc.append([])
            loss_arr.append([])
        old_loss = 0
        loss_count = 0
        for r in [args.iterations]:
            
            for e in tqdm(range(r)):
                # forward
                h = model(train_g.to('cuda:0'), (train_g.ndata['name-age'].float()).cuda())
                pos_score = pred(train_pos_g.to('cuda:0'), h.cuda())
                neg_score = pred(train_neg_g, h)
                loss = compute_loss(pos_score, neg_score)
                
                
                # backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if old_loss == round(loss.tolist(), 5):
                    #pass
                    #loss_count += 1
                    loss_count = 0
                else:
                    loss_count = 0
                if loss_count > 10:
                    loss_count = 0
                    #if e < .5*args.iterations:
                       # x -=1
                       # reset_count += 1
                    #r = args.iterations
                    #break
                #old_loss = round(loss.tolist(), 5)
                if e % 100 == 0:
                    print('In epoch {}, loss: {}'.format(e, loss))
                    from sklearn.metrics import roc_auc_score
                    with torch.no_grad():
                        pos_score = pred(test_pos_g, h).cpu()
                        neg_score = pred(test_neg_g, h).cpu()
                        auc[sample_iter].append(compute_auc(pos_score, neg_score))
                        loss_arr[sample_iter].append(loss)
                        print('AUC', e, compute_auc(pos_score, neg_score))

            # ----------- 5. check results ------------------------ #
            from sklearn.metrics import roc_auc_score
            with torch.no_grad():
                pos_score = pred(test_pos_g, h).cpu()
                neg_score = pred(test_neg_g, h).cpu()
                auc[sample_iter].append(compute_auc(pos_score, neg_score))
                print('AUC', e, compute_auc(pos_score, neg_score))
        x+=1

        #if reset_count > 15: 
            #pass

if reload_count > 5: 
    print("Compilation did not finish, consider adjusting your input we got stuck at a local minimum with loss = ", loss)
    

with torch.no_grad():
    pos_score = pred(test_pos_g, h)
    neg_score = pred(test_neg_g, h)



auc_resample = []
auc_resample.append(0)
aucsum = 0
for idx, x in enumerate(auc): 
    auc_resample.append(len(x) + aucsum)
    aucsum += len(x)

auc_total = [x for sublist in auc for x in sublist]

plt.figure(3, figsize=(15,10))
plt.axis([-10, auc_resample[len(auc_resample)-1] + 5, .4, 1])
for xc in auc_resample[:len(auc_resample)-1]: 
    line = plt.axvline(x = xc, color = 'r', linestyle='--')

plt.plot(auc_total, lw = 3)
plt.title("Area Under the Curve for " + dsm_type + " Link Prediction", size = 30)
plt.xlabel("100s of Iterations", size = 20)
plt.ylabel("AUC", size = 20)
plt.legend([line], ['Resampling of ' +  str(args.resample_rate*100) +'% of the dataset'], loc = 'upper left', fontsize = 20)
plt.savefig("determine_plots/" + dsm_type + "_" + str(int(args.resample_rate*100)) + "_" + str(args.iterations) + '_' + str(int(args.tsize*100)) + "_auc.jpg",  bbox_inches = 'tight')




columns = list(ncsr_age.columns)

dbatch = dgl.batch(dsm_all, ndata=['name-age'])

node_count = [[0]*len(columns)]*len(columns)


G = nx.DiGraph()
ndata = dbatch.ndata['name-age']
columns = list(ncsr_age.columns)


for x in columns: 
    G.add_node(x)



for idx, x in enumerate(test_pos_g.edges()[0]):
    from_node = columns[ndata[x][0]]
    to_node = columns[ndata[test_pos_g.edges()[1][idx]][0]]
    if G.has_edge(from_node, to_node):
        G[from_node][to_node]['w'] += pos_score[idx]
    else: 
        G.add_edges_from([(from_node, to_node)])
        G[from_node][to_node]['w'] = pos_score[idx]
    node_count[columns.index(from_node)][columns.index(to_node)] += 1



for x in columns: 
    for y in columns: 
        if G.has_edge(x, y):
            G[x][y]['w'] = G[x][y]['w']/node_count[columns.index(x)][columns.index(y)]


nx.write_gpickle(G, 'link_prediction_nx/DSM_'  + dsm_type + "_" + str(args.iterations) + '_' + str(int(args.resample_rate*100) ) + "-" + str(int(args.tsize*100)) + ".pkl")
