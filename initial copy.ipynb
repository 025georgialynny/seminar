{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "8ca28b1d433ae75eab343bd246052c2243632e123f07554f7cebf0e78e390e86"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.promodel.com/pdf/DES-ANN-Article-8-20-2018.pdf\n",
    "# Step one goal: create an ANN and find import variables, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as matplot\n",
    "import seaborn\n",
    "import bokeh\n",
    "#import keras\n",
    "import scipy\n",
    "import plotly\n",
    "import plotly.express as pe\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from get_tree_data import ncs1_data\n",
    "import re\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncs1 = ncs1_data()\n",
    "  # ncs1 holds all of the data from NCS 006693\n",
    "  # Includes ncs1.dxdm (DS0002 DataFrame) and ncs1.survey (DS001 DataFrame)\n",
    "  # Key Functions:\n",
    "    # ncs1.search_for_description looks for a descriotion of a column name in ncs1.dxdm or ncs1.survey\n",
    "  # Key Variables:\n",
    "    # ncs1.dxdm (DS0002)\n",
    "    # ncs1.survey (DS0001)\n",
    "    # ncs1.tree (Tree including descriptions of survey and dxdm columns)\n",
    "    # ncs1.root (root used in traversing ncs1.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_input = ncs1.survey.iloc[:, ncs1.root.loc[2,\"Start\"]+1:ncs1.root.loc[2, \"End\"]]\n",
    "keras_input.insert(0, 'CASEID', ncs1.survey.loc[:,'CASEID'], True)\n",
    "\n",
    "train_input = keras_input.sample(frac = .6)\n",
    "test_input = keras_input.drop(train_input.index)\n",
    "\n",
    "keras_output = ncs1.dxdm.loc[:, ('CASEID', 'DRGALT1')]\n",
    "\n",
    "train_output = keras_output.drop(test_input.index)\n",
    "test_output = keras_input.drop(train_input.index)\n",
    "\n",
    "# setup train/test input vars\n",
    "\n",
    "train_output = train_output.reset_index(drop=True)\n",
    "test_output = test_output.reset_index(drop=True)\n",
    "\n",
    "train_input = train_input.reset_index(drop=True)\n",
    "test_input = test_input.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      CASEID  V102  V103  V104  V105  V106  V107  V108  V109  V110  ...  V240  \\\n",
       "0      11025     2     1     2     7     1     0     5     5     0  ...     0   \n",
       "1      23759     2     1     2     8     2     0     5     5     0  ...     0   \n",
       "2      11463     3     3     1     7     1     0     5     5     0  ...     0   \n",
       "3      23333     2     3     5     6     1     0     1     0     1  ...     1   \n",
       "4      11408     2     1     2     6     3     0     3     1     0  ...     1   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "4854   25445     2     1     3     8     1     0     1     0     1  ...     1   \n",
       "4855   22120     3     1     1     8     1     0     1     0     1  ...     1   \n",
       "4856   25474     2     1     5     7     1     0     1     0     1  ...     5   \n",
       "4857   11415     5     5     1    10     2     0     3     5     0  ...     0   \n",
       "4858   22041     3     3     3     6     2     0     1     0     1  ...     1   \n",
       "\n",
       "      V241  V242  V243  V244  V245  V246  V247  V248  V249  \n",
       "0        0     0     1     1     0     9     2     4     4  \n",
       "1        0     0     1     2     0     1     3     4     4  \n",
       "2        0     0     1     2     0     3     3     3     4  \n",
       "3        1     3     0     0     2     3     1     4     4  \n",
       "4        1     1     0     0     1     4     1     4     4  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4854     1     3     0     0     2     2     2     2     4  \n",
       "4855     1     7     0     0     1     1     1     4     4  \n",
       "4856     0     0     0     0     3     2     3     1     4  \n",
       "4857     0     0     5     0     0     5     4     1     2  \n",
       "4858     1     3     0     0     2     2     2     4     4  \n",
       "\n",
       "[4859 rows x 63 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASEID</th>\n      <th>V102</th>\n      <th>V103</th>\n      <th>V104</th>\n      <th>V105</th>\n      <th>V106</th>\n      <th>V107</th>\n      <th>V108</th>\n      <th>V109</th>\n      <th>V110</th>\n      <th>...</th>\n      <th>V240</th>\n      <th>V241</th>\n      <th>V242</th>\n      <th>V243</th>\n      <th>V244</th>\n      <th>V245</th>\n      <th>V246</th>\n      <th>V247</th>\n      <th>V248</th>\n      <th>V249</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11025</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23759</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11463</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23333</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11408</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4854</th>\n      <td>25445</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4855</th>\n      <td>22120</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4856</th>\n      <td>25474</td>\n      <td>2</td>\n      <td>1</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4857</th>\n      <td>11415</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>10</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4858</th>\n      <td>22041</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>4859 rows × 63 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sections from ncs1.root and see what keras has to say about them in relation to DRGALT1 \n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=12, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.7594 - accuracy: 0.3886\n",
      "Epoch 2/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.4286 - accuracy: 0.8811\n",
      "Epoch 3/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3709 - accuracy: 0.8870\n",
      "Epoch 4/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3575 - accuracy: 0.8870\n",
      "Epoch 5/150\n",
      "486/486 [==============================] - 0s 998us/step - loss: 0.3564 - accuracy: 0.8858\n",
      "Epoch 6/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8861\n",
      "Epoch 7/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8837\n",
      "Epoch 8/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3584 - accuracy: 0.8842\n",
      "Epoch 9/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3572 - accuracy: 0.8849\n",
      "Epoch 10/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3437 - accuracy: 0.8914\n",
      "Epoch 11/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8828\n",
      "Epoch 12/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3602 - accuracy: 0.8834\n",
      "Epoch 13/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3602 - accuracy: 0.8834\n",
      "Epoch 14/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3530 - accuracy: 0.8869\n",
      "Epoch 15/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3518 - accuracy: 0.8875\n",
      "Epoch 16/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8758\n",
      "Epoch 17/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3588 - accuracy: 0.8841\n",
      "Epoch 18/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3465 - accuracy: 0.8901\n",
      "Epoch 19/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8889\n",
      "Epoch 20/150\n",
      "486/486 [==============================] - 0s 971us/step - loss: 0.3538 - accuracy: 0.8866\n",
      "Epoch 21/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8930\n",
      "Epoch 22/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3599 - accuracy: 0.8835\n",
      "Epoch 23/150\n",
      "486/486 [==============================] - 0s 957us/step - loss: 0.3472 - accuracy: 0.8898\n",
      "Epoch 24/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3550 - accuracy: 0.8859\n",
      "Epoch 25/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3626 - accuracy: 0.8822\n",
      "Epoch 26/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3556 - accuracy: 0.8856\n",
      "Epoch 27/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3414 - accuracy: 0.8925\n",
      "Epoch 28/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3643 - accuracy: 0.8813\n",
      "Epoch 29/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3634 - accuracy: 0.8816\n",
      "Epoch 30/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3732 - accuracy: 0.8769\n",
      "Epoch 31/150\n",
      "486/486 [==============================] - 0s 996us/step - loss: 0.3608 - accuracy: 0.8829\n",
      "Epoch 32/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8827\n",
      "Epoch 33/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3600 - accuracy: 0.8833\n",
      "Epoch 34/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3670 - accuracy: 0.8798\n",
      "Epoch 35/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8897\n",
      "Epoch 36/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3563 - accuracy: 0.8849\n",
      "Epoch 37/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3487 - accuracy: 0.8889\n",
      "Epoch 38/150\n",
      "486/486 [==============================] - 0s 942us/step - loss: 0.3659 - accuracy: 0.8804\n",
      "Epoch 39/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3621 - accuracy: 0.8819\n",
      "Epoch 40/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3594 - accuracy: 0.8836\n",
      "Epoch 41/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3686 - accuracy: 0.8784\n",
      "Epoch 42/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3526 - accuracy: 0.8871\n",
      "Epoch 43/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3602 - accuracy: 0.8829\n",
      "Epoch 44/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8925\n",
      "Epoch 45/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8856\n",
      "Epoch 46/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3605 - accuracy: 0.8824\n",
      "Epoch 47/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3554 - accuracy: 0.8851\n",
      "Epoch 48/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3701 - accuracy: 0.8780\n",
      "Epoch 49/150\n",
      "486/486 [==============================] - 0s 948us/step - loss: 0.3576 - accuracy: 0.8847\n",
      "Epoch 50/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3713 - accuracy: 0.8775\n",
      "Epoch 51/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3535 - accuracy: 0.8858\n",
      "Epoch 52/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8870\n",
      "Epoch 53/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.8850\n",
      "Epoch 54/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3447 - accuracy: 0.8905\n",
      "Epoch 55/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8877\n",
      "Epoch 56/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8890\n",
      "Epoch 57/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3648 - accuracy: 0.8809\n",
      "Epoch 58/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3684 - accuracy: 0.8791\n",
      "Epoch 59/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3608 - accuracy: 0.8823\n",
      "Epoch 60/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3467 - accuracy: 0.8891\n",
      "Epoch 61/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3532 - accuracy: 0.8870\n",
      "Epoch 62/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8848\n",
      "Epoch 63/150\n",
      "486/486 [==============================] - 0s 986us/step - loss: 0.3631 - accuracy: 0.8815\n",
      "Epoch 64/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8936\n",
      "Epoch 65/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3434 - accuracy: 0.8910\n",
      "Epoch 66/150\n",
      "486/486 [==============================] - 0s 924us/step - loss: 0.3679 - accuracy: 0.8792\n",
      "Epoch 67/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3592 - accuracy: 0.8833\n",
      "Epoch 68/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3714 - accuracy: 0.8766\n",
      "Epoch 69/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3700 - accuracy: 0.8775\n",
      "Epoch 70/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3583 - accuracy: 0.8832\n",
      "Epoch 71/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3590 - accuracy: 0.8833\n",
      "Epoch 72/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3521 - accuracy: 0.8865\n",
      "Epoch 73/150\n",
      "486/486 [==============================] - 0s 926us/step - loss: 0.3663 - accuracy: 0.8793\n",
      "Epoch 74/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3399 - accuracy: 0.8924\n",
      "Epoch 75/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3546 - accuracy: 0.8856\n",
      "Epoch 76/150\n",
      "486/486 [==============================] - 0s 986us/step - loss: 0.3465 - accuracy: 0.8890\n",
      "Epoch 77/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3589 - accuracy: 0.8835\n",
      "Epoch 78/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3418 - accuracy: 0.8918\n",
      "Epoch 79/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3516 - accuracy: 0.8871\n",
      "Epoch 80/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3623 - accuracy: 0.8823\n",
      "Epoch 81/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3543 - accuracy: 0.8861\n",
      "Epoch 82/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3606 - accuracy: 0.8827\n",
      "Epoch 83/150\n",
      "486/486 [==============================] - 0s 932us/step - loss: 0.3599 - accuracy: 0.8821\n",
      "Epoch 84/150\n",
      "486/486 [==============================] - 0s 920us/step - loss: 0.3448 - accuracy: 0.8905\n",
      "Epoch 85/150\n",
      "486/486 [==============================] - 0s 868us/step - loss: 0.3569 - accuracy: 0.8842\n",
      "Epoch 86/150\n",
      "486/486 [==============================] - 0s 946us/step - loss: 0.3538 - accuracy: 0.8856\n",
      "Epoch 87/150\n",
      "486/486 [==============================] - 0s 899us/step - loss: 0.3428 - accuracy: 0.8914\n",
      "Epoch 88/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3494 - accuracy: 0.8877\n",
      "Epoch 89/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8804\n",
      "Epoch 90/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8794\n",
      "Epoch 91/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3496 - accuracy: 0.8875\n",
      "Epoch 92/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8921\n",
      "Epoch 93/150\n",
      "486/486 [==============================] - 0s 928us/step - loss: 0.3446 - accuracy: 0.8902\n",
      "Epoch 94/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3423 - accuracy: 0.8915\n",
      "Epoch 95/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3541 - accuracy: 0.8858\n",
      "Epoch 96/150\n",
      "486/486 [==============================] - 0s 973us/step - loss: 0.3783 - accuracy: 0.8737\n",
      "Epoch 97/150\n",
      "486/486 [==============================] - 0s 965us/step - loss: 0.3544 - accuracy: 0.8854\n",
      "Epoch 98/150\n",
      "486/486 [==============================] - 0s 915us/step - loss: 0.3633 - accuracy: 0.8815\n",
      "Epoch 99/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3522 - accuracy: 0.8862\n",
      "Epoch 100/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3554 - accuracy: 0.8849\n",
      "Epoch 101/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3600 - accuracy: 0.8825\n",
      "Epoch 102/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3622 - accuracy: 0.8819\n",
      "Epoch 103/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3585 - accuracy: 0.8835\n",
      "Epoch 104/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3527 - accuracy: 0.8857\n",
      "Epoch 105/150\n",
      "486/486 [==============================] - 0s 953us/step - loss: 0.3677 - accuracy: 0.8793\n",
      "Epoch 106/150\n",
      "486/486 [==============================] - 0s 925us/step - loss: 0.3494 - accuracy: 0.8881\n",
      "Epoch 107/150\n",
      "486/486 [==============================] - 0s 992us/step - loss: 0.3630 - accuracy: 0.8812\n",
      "Epoch 108/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3658 - accuracy: 0.8804\n",
      "Epoch 109/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8850\n",
      "Epoch 110/150\n",
      "486/486 [==============================] - 0s 965us/step - loss: 0.3507 - accuracy: 0.8871\n",
      "Epoch 111/150\n",
      "486/486 [==============================] - 0s 975us/step - loss: 0.3642 - accuracy: 0.8800\n",
      "Epoch 112/150\n",
      "486/486 [==============================] - 0s 913us/step - loss: 0.3506 - accuracy: 0.8879\n",
      "Epoch 113/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3603 - accuracy: 0.8824\n",
      "Epoch 114/150\n",
      "486/486 [==============================] - 1s 2ms/step - loss: 0.3650 - accuracy: 0.8806\n",
      "Epoch 115/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3667 - accuracy: 0.8802\n",
      "Epoch 116/150\n",
      "486/486 [==============================] - 1s 2ms/step - loss: 0.3554 - accuracy: 0.8845\n",
      "Epoch 117/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3632 - accuracy: 0.8810\n",
      "Epoch 118/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3671 - accuracy: 0.8791\n",
      "Epoch 119/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3791 - accuracy: 0.8737\n",
      "Epoch 120/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3593 - accuracy: 0.8835\n",
      "Epoch 121/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3708 - accuracy: 0.8776\n",
      "Epoch 122/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3567 - accuracy: 0.8846\n",
      "Epoch 123/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3564 - accuracy: 0.8846\n",
      "Epoch 124/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3490 - accuracy: 0.8881\n",
      "Epoch 125/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3597 - accuracy: 0.8829\n",
      "Epoch 126/150\n",
      "486/486 [==============================] - 0s 959us/step - loss: 0.3543 - accuracy: 0.8852\n",
      "Epoch 127/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3744 - accuracy: 0.8757\n",
      "Epoch 128/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3511 - accuracy: 0.8869\n",
      "Epoch 129/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3708 - accuracy: 0.8782\n",
      "Epoch 130/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3598 - accuracy: 0.8832\n",
      "Epoch 131/150\n",
      "486/486 [==============================] - 1s 2ms/step - loss: 0.3613 - accuracy: 0.8823\n",
      "Epoch 132/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3533 - accuracy: 0.8863\n",
      "Epoch 133/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3574 - accuracy: 0.8845\n",
      "Epoch 134/150\n",
      "486/486 [==============================] - 1s 2ms/step - loss: 0.3603 - accuracy: 0.8815\n",
      "Epoch 135/150\n",
      "486/486 [==============================] - 1s 2ms/step - loss: 0.3695 - accuracy: 0.8773\n",
      "Epoch 136/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3632 - accuracy: 0.8813\n",
      "Epoch 137/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3646 - accuracy: 0.8803\n",
      "Epoch 138/150\n",
      "486/486 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8913\n",
      "Epoch 139/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3496 - accuracy: 0.8873\n",
      "Epoch 140/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3574 - accuracy: 0.8840\n",
      "Epoch 141/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3457 - accuracy: 0.8892\n",
      "Epoch 142/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3595 - accuracy: 0.8837\n",
      "Epoch 143/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3503 - accuracy: 0.8873\n",
      "Epoch 144/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3573 - accuracy: 0.8841\n",
      "Epoch 145/150\n",
      "486/486 [==============================] - 0s 971us/step - loss: 0.3496 - accuracy: 0.8874\n",
      "Epoch 146/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3560 - accuracy: 0.8850\n",
      "Epoch 147/150\n",
      "486/486 [==============================] - 0s 1000us/step - loss: 0.3482 - accuracy: 0.8884\n",
      "Epoch 148/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3576 - accuracy: 0.8837\n",
      "Epoch 149/150\n",
      "486/486 [==============================] - 0s 922us/step - loss: 0.3537 - accuracy: 0.8859\n",
      "Epoch 150/150\n",
      "486/486 [==============================] - 1s 1ms/step - loss: 0.3558 - accuracy: 0.8845\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1791f29f7c0>"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "model.fit(train_input.iloc[:, 1:13], train_output.iloc[:, 1], epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "152/152 [==============================] - 0s 974us/step - loss: 0.3561 - accuracy: 0.8854\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(train_input.iloc[:, 1:13], train_output.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_input.iloc[:, 1:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] == test_output.iloc[i, 1]:\n",
    "        right = right+1\n",
    "print(\"accuracy: \" + str(right/len(test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}